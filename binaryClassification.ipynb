{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visual Recognition Mini Project\n",
    "<h4> Varsha Yamsani(IMT2022506)<br>\n",
    "<h4> Keshav Goyal(IMT2022560)<br>\n",
    "<h4> R Harshavardhan(IMT2022515)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task A and B</h3>\n",
    "<h5>In this section, we explore two approaches for classifying faces as \"with mask\" or \"without mask.\" The first method involves extracting handcrafted features and using traditional machine learning classifiers like SVM and Neural Networks. The second approach leverages the power of Convolutional Neural Networks (CNNs) to automatically learn features from images. We will train and evaluate both methods, experiment with hyperparameter tuning, and compare their performance to determine which approach is more effective for face mask classification.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath = \"dataset\"  # Define the main dataset directory\n",
    "images, labels = [], []  # Initialize lists to store images and corresponding labels\n",
    "\n",
    "# Loop through both categories: \"with_mask\" and \"without_mask\"\n",
    "for category in [\"with_mask\", \"without_mask\"]:  \n",
    "    path = os.path.join(datasetPath, category)  # Construct the full path to category folder\n",
    "    label = 1 if category == \"with_mask\" else 0  # Assign labels: 1 for \"with_mask\", 0 for \"without_mask\"\n",
    "\n",
    "    # Iterate through all image files in the category folder\n",
    "    for file in os.listdir(path):  \n",
    "        img_path = os.path.join(path, file)  # Construct full path to the image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\n",
    "        \n",
    "        if img is None:  # Handle case where image file could not be read\n",
    "            print(f\"Warning: Could not read {img_path}\")\n",
    "            continue  # Skip to the next image\n",
    "        \n",
    "        img = cv2.resize(img, (64, 64))  # Resize image to 64x64 pixels\n",
    "        images.append(img)  # Append the processed image to the list\n",
    "        labels.append(label)  # Append the corresponding label  \n",
    "\n",
    "# Convert lists to NumPy arrays for further processing\n",
    "X, y = np.array(images), np.array(labels)\n",
    "\n",
    "# Prepare input data for CNN by normalizing pixel values (0-1) and reshaping for CNN input\n",
    "xCNN = X.reshape(-1, 64, 64, 1) / 255.0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "with_mask: 2165 images\n",
      "without_mask: 1930 images\n",
      "Total images: 4095\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define dataset path\n",
    "datasetPath = \"dataset\"\n",
    "images, labels = [], []\n",
    "dataset_structure = {}\n",
    "\n",
    "# Loop through categories: \"with_mask\" and \"without_mask\"\n",
    "for category in [\"with_mask\", \"without_mask\"]:  \n",
    "    path = os.path.join(datasetPath, category)  # Construct category path\n",
    "    label = 1 if category == \"with_mask\" else 0  # Assign labels\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: Directory {path} does not exist!\")\n",
    "        continue\n",
    "    \n",
    "    file_list = os.listdir(path)  # List files in the category folder\n",
    "    dataset_structure[category] = len(file_list)  # Store category count\n",
    "    \n",
    "    for file in file_list:  \n",
    "        img_path = os.path.join(path, file)  # Full image path\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale\n",
    "        \n",
    "        if img is None:  # Handle unreadable files\n",
    "            print(f\"Warning: Could not read {img_path}\")\n",
    "            continue  \n",
    "        \n",
    "        img = cv2.resize(img, (64, 64))  # Resize image\n",
    "        images.append(img)  # Store image\n",
    "        labels.append(label)  # Store label\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X, y = np.array(images), np.array(labels)\n",
    "\n",
    "# Normalize and reshape input for CNN\n",
    "xCNN = X.reshape(-1, 64, 64, 1) / 255.0\n",
    "\n",
    "# Print dataset structure\n",
    "print(\"Dataset Structure:\")\n",
    "for category, count in dataset_structure.items():\n",
    "    print(f\"{category}: {count} images\")\n",
    "print(f\"Total images: {len(X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task A: Binary Classification using Handcrafted features and ML Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-processing the data for task A, which means extracting handcrafted features and concatenating thmem into a single feature vector to store them in a list for processing. This includes scaling the data and splitting the data into a train and test frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []  # List to store feature vectors\n",
    "for img in images:\n",
    "    # HOG (Histogram of Oriented Gradients) Feature Extraction\n",
    "    hogFeatures = hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), feature_vector=True)\n",
    "\n",
    "    # Sobel Edge Detection (Gradient in x and y directions)\n",
    "    sobelX = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)  # Sobel filter in x-direction\n",
    "    sobelY = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)  # Sobel filter in y-direction\n",
    "    sobelCombined = cv2.magnitude(sobelX, sobelY).flatten()  # Compute gradient magnitude and flatten\n",
    "\n",
    "    # Canny Edge Detection\n",
    "    canny_edges = cv2.Canny(img, 100, 200).flatten()  # Apply Canny edge detection and flatten\n",
    "\n",
    "    # Concatenate all features into a single vector\n",
    "    feature_vector = np.hstack([hogFeatures, sobelCombined, canny_edges])\n",
    "    features.append(feature_vector)  # Append feature vector to list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xFeatures = np.array(features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(xFeatures, y, test_size=0.2, random_state=42) #Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "xTrain = scaler.fit_transform(xTrain)  # Fit and transform training data\n",
    "xTest = scaler.transform(xTest)  # Transform test data using the same scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"linear\")  # Use a linear kernel for SVM\n",
    "svm.fit(xTrain, yTrain)  # Train the SVM model\n",
    "yPredSVM = svm.predict(xTest)  # Make predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MLP Model(Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Network (MLP - Multi-Layer Perceptron)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64,32), max_iter=300)  # Define an MLP with 1 hidden layer of 100 neurons\n",
    "mlp.fit(xTrain, yTrain)  # Train the MLP model\n",
    "yPredMLP = mlp.predict(xTest)  # Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.884004884004884\n",
      "MLP Accuracy: 0.9194139194139194\n",
      "\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       366\n",
      "           1       0.89      0.90      0.90       453\n",
      "\n",
      "    accuracy                           0.88       819\n",
      "   macro avg       0.88      0.88      0.88       819\n",
      "weighted avg       0.88      0.88      0.88       819\n",
      "\n",
      "\n",
      "MLP Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       366\n",
      "           1       0.91      0.95      0.93       453\n",
      "\n",
      "    accuracy                           0.92       819\n",
      "   macro avg       0.92      0.92      0.92       819\n",
      "weighted avg       0.92      0.92      0.92       819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Accuracy:\", accuracy_score(yTest, yPredSVM))  # Print accuracy of SVM\n",
    "print(\"MLP Accuracy:\", accuracy_score(yTest, yPredMLP))  # Print accuracy of MLP\n",
    "\n",
    "print(\"\\nSVM Classification Report:\\n\", classification_report(yTest, yPredSVM))\n",
    "print(\"\\nMLP Classification Report:\\n\", classification_report(yTest, yPredMLP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task B: Using CNN for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Splitting into train test for CNN and building a CNN model with different hyperparameters, like checking various optimizers, dropout values and learning rates. We also try two activation functions: Sigmoid and tanh to get different results. Various depths of layers and number of epochs were also tried. Here are few of the cases included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainCNN, xTestCNN, yTrainCNN, yTestCNN = train_test_split(xCNN, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Using SIGMOID activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN with optimizer=adam, dropout=0.3, learning_rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rog\\anaconda3\\envs\\MLTutorial\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.6327 - loss: 0.6183 - val_accuracy: 0.8596 - val_loss: 0.3607 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8578 - loss: 0.3460 - val_accuracy: 0.9219 - val_loss: 0.2189 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9049 - loss: 0.2258 - val_accuracy: 0.9402 - val_loss: 0.1695 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9344 - loss: 0.1653 - val_accuracy: 0.9243 - val_loss: 0.1965 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9422 - loss: 0.1432 - val_accuracy: 0.9512 - val_loss: 0.1385 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9634 - loss: 0.1011 - val_accuracy: 0.9341 - val_loss: 0.1637 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9752 - loss: 0.0754 - val_accuracy: 0.9377 - val_loss: 0.1810 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9703 - loss: 0.0733 - val_accuracy: 0.9475 - val_loss: 0.1739 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9877 - loss: 0.0365 - val_accuracy: 0.9560 - val_loss: 0.1480 - learning_rate: 2.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9933 - loss: 0.0253 - val_accuracy: 0.9609 - val_loss: 0.1328 - learning_rate: 2.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9954 - loss: 0.0184 - val_accuracy: 0.9597 - val_loss: 0.1405 - learning_rate: 2.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9979 - loss: 0.0151 - val_accuracy: 0.9597 - val_loss: 0.1377 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9981 - loss: 0.0139 - val_accuracy: 0.9621 - val_loss: 0.1417 - learning_rate: 2.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0116 - val_accuracy: 0.9621 - val_loss: 0.1465 - learning_rate: 4.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0111 - val_accuracy: 0.9634 - val_loss: 0.1490 - learning_rate: 4.0000e-05\n",
      "Training CNN with optimizer=adam, dropout=0.3, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.6277 - loss: 0.6417 - val_accuracy: 0.7937 - val_loss: 0.4776 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.7799 - loss: 0.4714 - val_accuracy: 0.8584 - val_loss: 0.3185 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8650 - loss: 0.3208 - val_accuracy: 0.8803 - val_loss: 0.2704 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8938 - loss: 0.2371 - val_accuracy: 0.9206 - val_loss: 0.2080 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9217 - loss: 0.2045 - val_accuracy: 0.9292 - val_loss: 0.1743 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9472 - loss: 0.1389 - val_accuracy: 0.9524 - val_loss: 0.1402 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9627 - loss: 0.1055 - val_accuracy: 0.9414 - val_loss: 0.1394 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9691 - loss: 0.0822 - val_accuracy: 0.9499 - val_loss: 0.1590 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9739 - loss: 0.0754 - val_accuracy: 0.9475 - val_loss: 0.1447 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9823 - loss: 0.0555 - val_accuracy: 0.9499 - val_loss: 0.1577 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9918 - loss: 0.0344 - val_accuracy: 0.9597 - val_loss: 0.1383 - learning_rate: 2.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9949 - loss: 0.0217 - val_accuracy: 0.9585 - val_loss: 0.1412 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9975 - loss: 0.0156 - val_accuracy: 0.9646 - val_loss: 0.1507 - learning_rate: 2.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9962 - loss: 0.0186 - val_accuracy: 0.9634 - val_loss: 0.1497 - learning_rate: 2.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9958 - loss: 0.0135 - val_accuracy: 0.9621 - val_loss: 0.1516 - learning_rate: 4.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9936 - loss: 0.0167 - val_accuracy: 0.9609 - val_loss: 0.1550 - learning_rate: 4.0000e-05\n",
      "Training CNN with optimizer=adam, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.5894 - loss: 0.6383 - val_accuracy: 0.7277 - val_loss: 0.5268 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7500 - loss: 0.5119 - val_accuracy: 0.8620 - val_loss: 0.3391 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8664 - loss: 0.3418 - val_accuracy: 0.9109 - val_loss: 0.2359 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8998 - loss: 0.2637 - val_accuracy: 0.9194 - val_loss: 0.1920 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9181 - loss: 0.2145 - val_accuracy: 0.9255 - val_loss: 0.1751 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9357 - loss: 0.1687 - val_accuracy: 0.9426 - val_loss: 0.1485 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9549 - loss: 0.1360 - val_accuracy: 0.9316 - val_loss: 0.1679 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9469 - loss: 0.1339 - val_accuracy: 0.9536 - val_loss: 0.1353 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9728 - loss: 0.0835 - val_accuracy: 0.9426 - val_loss: 0.1643 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9686 - loss: 0.0811 - val_accuracy: 0.9365 - val_loss: 0.1828 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9745 - loss: 0.0601 - val_accuracy: 0.9341 - val_loss: 0.1759 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9766 - loss: 0.0595 - val_accuracy: 0.9609 - val_loss: 0.1171 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9931 - loss: 0.0292 - val_accuracy: 0.9609 - val_loss: 0.1216 - learning_rate: 2.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9958 - loss: 0.0194 - val_accuracy: 0.9597 - val_loss: 0.1317 - learning_rate: 2.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9927 - loss: 0.0249 - val_accuracy: 0.9621 - val_loss: 0.1291 - learning_rate: 2.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9958 - loss: 0.0202 - val_accuracy: 0.9621 - val_loss: 0.1301 - learning_rate: 4.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9944 - loss: 0.0189 - val_accuracy: 0.9646 - val_loss: 0.1286 - learning_rate: 4.0000e-05\n",
      "Training CNN with optimizer=adam, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.6168 - loss: 0.6305 - val_accuracy: 0.7167 - val_loss: 0.5427 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.7690 - loss: 0.4940 - val_accuracy: 0.8449 - val_loss: 0.3710 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.8591 - loss: 0.3178 - val_accuracy: 0.8584 - val_loss: 0.3020 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8975 - loss: 0.2491 - val_accuracy: 0.9341 - val_loss: 0.1813 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9389 - loss: 0.1742 - val_accuracy: 0.8669 - val_loss: 0.3158 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9308 - loss: 0.1653 - val_accuracy: 0.9487 - val_loss: 0.1453 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9548 - loss: 0.1221 - val_accuracy: 0.9426 - val_loss: 0.1493 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9609 - loss: 0.1052 - val_accuracy: 0.9438 - val_loss: 0.1578 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9713 - loss: 0.0859 - val_accuracy: 0.9597 - val_loss: 0.1499 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9829 - loss: 0.0473 - val_accuracy: 0.9573 - val_loss: 0.1555 - learning_rate: 2.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9891 - loss: 0.0425 - val_accuracy: 0.9683 - val_loss: 0.1320 - learning_rate: 2.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9895 - loss: 0.0322 - val_accuracy: 0.9670 - val_loss: 0.1342 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9904 - loss: 0.0316 - val_accuracy: 0.9646 - val_loss: 0.1383 - learning_rate: 2.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9936 - loss: 0.0266 - val_accuracy: 0.9658 - val_loss: 0.1408 - learning_rate: 2.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9939 - loss: 0.0270 - val_accuracy: 0.9646 - val_loss: 0.1401 - learning_rate: 4.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9914 - loss: 0.0284 - val_accuracy: 0.9658 - val_loss: 0.1401 - learning_rate: 4.0000e-05\n",
      "Training CNN with optimizer=sgd, dropout=0.3, learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.5290 - loss: 0.6911 - val_accuracy: 0.5507 - val_loss: 0.6851 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5566 - loss: 0.6855 - val_accuracy: 0.5397 - val_loss: 0.6794 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5589 - loss: 0.6784 - val_accuracy: 0.5311 - val_loss: 0.6701 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5430 - loss: 0.6710 - val_accuracy: 0.6081 - val_loss: 0.6583 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5783 - loss: 0.6542 - val_accuracy: 0.6239 - val_loss: 0.6358 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6498 - loss: 0.6298 - val_accuracy: 0.6471 - val_loss: 0.6238 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6647 - loss: 0.6081 - val_accuracy: 0.6618 - val_loss: 0.5906 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6558 - loss: 0.5876 - val_accuracy: 0.6606 - val_loss: 0.5723 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6620 - loss: 0.5804 - val_accuracy: 0.6190 - val_loss: 0.6272 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6968 - loss: 0.5550 - val_accuracy: 0.7155 - val_loss: 0.5392 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7201 - loss: 0.5369 - val_accuracy: 0.7045 - val_loss: 0.5330 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7242 - loss: 0.5357 - val_accuracy: 0.7521 - val_loss: 0.5102 - learning_rate: 0.0100\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7263 - loss: 0.5171 - val_accuracy: 0.7070 - val_loss: 0.5184 - learning_rate: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7558 - loss: 0.4962 - val_accuracy: 0.7460 - val_loss: 0.5063 - learning_rate: 0.0100\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7557 - loss: 0.4860 - val_accuracy: 0.7668 - val_loss: 0.4797 - learning_rate: 0.0100\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7558 - loss: 0.4942 - val_accuracy: 0.7766 - val_loss: 0.4775 - learning_rate: 0.0100\n",
      "Epoch 17/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7532 - loss: 0.4951 - val_accuracy: 0.6972 - val_loss: 0.5285 - learning_rate: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7689 - loss: 0.4762 - val_accuracy: 0.8144 - val_loss: 0.4384 - learning_rate: 0.0100\n",
      "Epoch 19/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7906 - loss: 0.4505 - val_accuracy: 0.7473 - val_loss: 0.4776 - learning_rate: 0.0100\n",
      "Epoch 20/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7768 - loss: 0.4652 - val_accuracy: 0.6886 - val_loss: 0.5663 - learning_rate: 0.0100\n",
      "Training CNN with optimizer=sgd, dropout=0.3, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.5224 - loss: 0.6915 - val_accuracy: 0.5641 - val_loss: 0.6859 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5660 - loss: 0.6850 - val_accuracy: 0.5531 - val_loss: 0.6776 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5415 - loss: 0.6787 - val_accuracy: 0.6740 - val_loss: 0.6730 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5830 - loss: 0.6716 - val_accuracy: 0.5543 - val_loss: 0.6574 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6202 - loss: 0.6588 - val_accuracy: 0.5238 - val_loss: 0.6388 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6020 - loss: 0.6432 - val_accuracy: 0.5458 - val_loss: 0.6420 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6542 - loss: 0.6181 - val_accuracy: 0.6801 - val_loss: 0.5989 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6924 - loss: 0.5865 - val_accuracy: 0.6496 - val_loss: 0.6081 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6971 - loss: 0.5777 - val_accuracy: 0.6947 - val_loss: 0.5591 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7255 - loss: 0.5360 - val_accuracy: 0.6679 - val_loss: 0.5789 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7352 - loss: 0.5275 - val_accuracy: 0.6716 - val_loss: 0.5676 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7516 - loss: 0.4917 - val_accuracy: 0.7424 - val_loss: 0.5119 - learning_rate: 0.0100\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.7768 - loss: 0.4696 - val_accuracy: 0.7692 - val_loss: 0.4538 - learning_rate: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7800 - loss: 0.4534 - val_accuracy: 0.7753 - val_loss: 0.4615 - learning_rate: 0.0100\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8071 - loss: 0.4258 - val_accuracy: 0.8291 - val_loss: 0.4048 - learning_rate: 0.0100\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8058 - loss: 0.4214 - val_accuracy: 0.8144 - val_loss: 0.3958 - learning_rate: 0.0100\n",
      "Epoch 17/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8323 - loss: 0.3976 - val_accuracy: 0.8339 - val_loss: 0.3909 - learning_rate: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8201 - loss: 0.3966 - val_accuracy: 0.8413 - val_loss: 0.3746 - learning_rate: 0.0100\n",
      "Epoch 19/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8273 - loss: 0.3790 - val_accuracy: 0.8608 - val_loss: 0.3634 - learning_rate: 0.0100\n",
      "Epoch 20/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8390 - loss: 0.3632 - val_accuracy: 0.8449 - val_loss: 0.3636 - learning_rate: 0.0100\n",
      "Training CNN with optimizer=sgd, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.5081 - loss: 0.6939 - val_accuracy: 0.5861 - val_loss: 0.6879 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5509 - loss: 0.6880 - val_accuracy: 0.5556 - val_loss: 0.6833 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5664 - loss: 0.6820 - val_accuracy: 0.5958 - val_loss: 0.6771 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.5905 - loss: 0.6775 - val_accuracy: 0.6288 - val_loss: 0.6682 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.6042 - loss: 0.6658 - val_accuracy: 0.6447 - val_loss: 0.6630 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.6227 - loss: 0.6542 - val_accuracy: 0.5482 - val_loss: 0.6352 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.6264 - loss: 0.6347 - val_accuracy: 0.5824 - val_loss: 0.6158 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6606 - loss: 0.6111 - val_accuracy: 0.6789 - val_loss: 0.5917 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6688 - loss: 0.5896 - val_accuracy: 0.6117 - val_loss: 0.6570 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6659 - loss: 0.6004 - val_accuracy: 0.6520 - val_loss: 0.6016 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7046 - loss: 0.5539 - val_accuracy: 0.7143 - val_loss: 0.5492 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7080 - loss: 0.5511 - val_accuracy: 0.6825 - val_loss: 0.5556 - learning_rate: 0.0100\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7385 - loss: 0.5188 - val_accuracy: 0.7497 - val_loss: 0.5094 - learning_rate: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7344 - loss: 0.5138 - val_accuracy: 0.7216 - val_loss: 0.5197 - learning_rate: 0.0100\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7454 - loss: 0.5080 - val_accuracy: 0.8059 - val_loss: 0.4729 - learning_rate: 0.0100\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7535 - loss: 0.4898 - val_accuracy: 0.7363 - val_loss: 0.5015 - learning_rate: 0.0100\n",
      "Epoch 17/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7775 - loss: 0.4718 - val_accuracy: 0.8010 - val_loss: 0.4435 - learning_rate: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7766 - loss: 0.4634 - val_accuracy: 0.8376 - val_loss: 0.4227 - learning_rate: 0.0100\n",
      "Epoch 19/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7915 - loss: 0.4401 - val_accuracy: 0.8120 - val_loss: 0.4322 - learning_rate: 0.0100\n",
      "Epoch 20/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7943 - loss: 0.4400 - val_accuracy: 0.8303 - val_loss: 0.4036 - learning_rate: 0.0100\n",
      "Training CNN with optimizer=sgd, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5002 - loss: 0.6925 - val_accuracy: 0.5543 - val_loss: 0.6861 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5550 - loss: 0.6854 - val_accuracy: 0.5519 - val_loss: 0.6820 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.5578 - loss: 0.6835 - val_accuracy: 0.6349 - val_loss: 0.6782 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5830 - loss: 0.6768 - val_accuracy: 0.6667 - val_loss: 0.6709 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5935 - loss: 0.6686 - val_accuracy: 0.5470 - val_loss: 0.6552 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5729 - loss: 0.6562 - val_accuracy: 0.6508 - val_loss: 0.6443 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.6426 - loss: 0.6395 - val_accuracy: 0.5421 - val_loss: 0.6227 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6512 - loss: 0.6120 - val_accuracy: 0.6129 - val_loss: 0.6622 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6580 - loss: 0.5984 - val_accuracy: 0.6899 - val_loss: 0.5700 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6823 - loss: 0.5644 - val_accuracy: 0.6935 - val_loss: 0.5599 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6955 - loss: 0.5577 - val_accuracy: 0.7399 - val_loss: 0.5304 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7249 - loss: 0.5346 - val_accuracy: 0.7375 - val_loss: 0.5141 - learning_rate: 0.0100\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7198 - loss: 0.5348 - val_accuracy: 0.6874 - val_loss: 0.5220 - learning_rate: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7347 - loss: 0.5193 - val_accuracy: 0.7167 - val_loss: 0.5209 - learning_rate: 0.0100\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7679 - loss: 0.4750 - val_accuracy: 0.8034 - val_loss: 0.4705 - learning_rate: 0.0100\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7771 - loss: 0.4761 - val_accuracy: 0.6801 - val_loss: 0.5459 - learning_rate: 0.0100\n",
      "Epoch 17/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7638 - loss: 0.4766 - val_accuracy: 0.8278 - val_loss: 0.4291 - learning_rate: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7888 - loss: 0.4513 - val_accuracy: 0.7912 - val_loss: 0.4291 - learning_rate: 0.0100\n",
      "Epoch 19/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8047 - loss: 0.4250 - val_accuracy: 0.7888 - val_loss: 0.4428 - learning_rate: 0.0100\n",
      "Epoch 20/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7995 - loss: 0.4431 - val_accuracy: 0.8339 - val_loss: 0.3959 - learning_rate: 0.0100\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(optimizer='adam', dropout_rate=0.5, learning_rate=1e-3):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 1)),  # First convolutional layer\n",
    "        MaxPooling2D(2,2),  # Pooling to reduce spatial dimensions\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu'),  # Second convolutional layer\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Conv2D(128, (3,3), activation='relu'),  # Third convolutional layer\n",
    "        MaxPooling2D(2,2),\n",
    "        \n",
    "        Flatten(),  # Flatten feature maps into a single vector\n",
    "        Dense(128, activation='relu'),  # Fully connected layer\n",
    "        Dropout(dropout_rate),  # Dropout to reduce overfitting\n",
    "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer! Choose 'adam' or 'sgd'.\")\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train CNN with different hyperparameters\n",
    "optimizers = ['adam', 'sgd']  # Optimizer variations\n",
    "dropoutValues = [0.3, 0.5]  # Dropout variations\n",
    "learningRates = [1e-3, 1e-4]  # Learning rate variations\n",
    "\n",
    "for opt in optimizers:\n",
    "    for dropout in dropoutValues:\n",
    "        for learning in learningRates:\n",
    "            print(f\"Training CNN with optimizer={opt}, dropout={dropout}, learning_rate={learning}\")\n",
    "            model = build_cnn(optimizer=opt, dropout_rate=dropout, learning_rate=learning)\n",
    "\n",
    "            # Callbacks: Stop training early if no improvement & adjust learning rate\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
    "            ]\n",
    "\n",
    "        # Train the model\n",
    "            model.fit(xTrainCNN,yTrainCNN,  validation_data=(xTestCNN, yTestCNN), epochs=20, callbacks=callbacks, verbose=1)\n",
    "\n",
    "        # Save the trained model\n",
    "            model.save(f'face_mask_classifier_{opt}_{dropout}_{learning}.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9612 - loss: 0.1386\n",
      "CNN (Optimizer=adam, Dropout=0.3, learning = 0.001) Accuracy: 0.9609\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9596 - loss: 0.1285\n",
      "CNN (Optimizer=adam, Dropout=0.3, learning = 0.0001) Accuracy: 0.9597\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9594 - loss: 0.1118\n",
      "CNN (Optimizer=adam, Dropout=0.5, learning = 0.001) Accuracy: 0.9609\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9691 - loss: 0.1318\n",
      "CNN (Optimizer=adam, Dropout=0.5, learning = 0.0001) Accuracy: 0.9683\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8055 - loss: 0.4647\n",
      "CNN (Optimizer=sgd, Dropout=0.3, learning = 0.001) Accuracy: 0.8144\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8412 - loss: 0.3859\n",
      "CNN (Optimizer=sgd, Dropout=0.3, learning = 0.0001) Accuracy: 0.8608\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8100 - loss: 0.4253\n",
      "CNN (Optimizer=sgd, Dropout=0.5, learning = 0.001) Accuracy: 0.8303\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8129 - loss: 0.4180\n",
      "CNN (Optimizer=sgd, Dropout=0.5, learning = 0.0001) Accuracy: 0.8339\n"
     ]
    }
   ],
   "source": [
    "for opt in optimizers:\n",
    "    for dropout in dropoutValues:\n",
    "        for learning in learningRates:\n",
    "            model = keras.models.load_model(f'face_mask_classifier_{opt}_{dropout}_{learning}.keras')\n",
    "            testLoss, testAccuracy = model.evaluate(xTestCNN, yTestCNN)\n",
    "            print(f\"CNN (Optimizer={opt}, Dropout={dropout}, learning = {learning}) Accuracy: {testAccuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using tanh in Output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN with optimizer=adam, dropout=0.3, learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.5566 - loss: 1.1589 - val_accuracy: 0.7070 - val_loss: 0.5370 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7755 - loss: 0.4627 - val_accuracy: 0.8791 - val_loss: 0.3140 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6865 - loss: 0.9311 - val_accuracy: 0.8266 - val_loss: 0.4184 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8438 - loss: 0.4044 - val_accuracy: 0.8840 - val_loss: 0.3019 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8716 - loss: 0.4328 - val_accuracy: 0.5678 - val_loss: 0.9485 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.6778 - loss: 0.6999 - val_accuracy: 0.7668 - val_loss: 0.4630 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8045 - loss: 0.4384 - val_accuracy: 0.7924 - val_loss: 0.4072 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8588 - loss: 0.3629 - val_accuracy: 0.8584 - val_loss: 0.3552 - learning_rate: 2.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.8518 - loss: 0.3660 - val_accuracy: 0.8608 - val_loss: 0.3268 - learning_rate: 2.0000e-04\n",
      "Training CNN with optimizer=adam, dropout=0.3, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.5638 - loss: 0.9287 - val_accuracy: 0.7277 - val_loss: 0.5818 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7522 - loss: 0.5389 - val_accuracy: 0.8120 - val_loss: 0.4016 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7372 - loss: 0.5498 - val_accuracy: 0.8254 - val_loss: 0.3775 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8483 - loss: 0.3599 - val_accuracy: 0.8913 - val_loss: 0.2762 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9130 - loss: 0.2264 - val_accuracy: 0.8620 - val_loss: 0.3126 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9090 - loss: 0.2369 - val_accuracy: 0.9158 - val_loss: 0.2236 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.8968 - loss: 0.2467 - val_accuracy: 0.9267 - val_loss: 0.1957 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9415 - loss: 0.1712 - val_accuracy: 0.9377 - val_loss: 0.1785 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9574 - loss: 0.1269 - val_accuracy: 0.9243 - val_loss: 0.1719 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9592 - loss: 0.1337 - val_accuracy: 0.9548 - val_loss: 0.1408 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9705 - loss: 0.0836 - val_accuracy: 0.9475 - val_loss: 0.1519 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9724 - loss: 0.0751 - val_accuracy: 0.9463 - val_loss: 0.2414 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9856 - loss: 0.0538 - val_accuracy: 0.9353 - val_loss: 0.4518 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9801 - loss: 0.0878 - val_accuracy: 0.9573 - val_loss: 0.2060 - learning_rate: 2.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9957 - loss: 0.0238 - val_accuracy: 0.9597 - val_loss: 0.1842 - learning_rate: 2.0000e-04\n",
      "Training CNN with optimizer=adam, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.5633 - loss: 1.2735 - val_accuracy: 0.7106 - val_loss: 0.5381 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7655 - loss: 0.4992 - val_accuracy: 0.8413 - val_loss: 0.3675 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8524 - loss: 0.3469 - val_accuracy: 0.8950 - val_loss: 0.2805 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8793 - loss: 0.2906 - val_accuracy: 0.9023 - val_loss: 0.2610 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9231 - loss: 0.2417 - val_accuracy: 0.9365 - val_loss: 0.1881 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8758 - loss: 0.3204 - val_accuracy: 0.9096 - val_loss: 0.2649 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8472 - loss: 0.3604 - val_accuracy: 0.9389 - val_loss: 0.1898 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9304 - loss: 0.1691 - val_accuracy: 0.9499 - val_loss: 0.1897 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9663 - loss: 0.1116 - val_accuracy: 0.9487 - val_loss: 0.1560 - learning_rate: 2.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9647 - loss: 0.1059 - val_accuracy: 0.9512 - val_loss: 0.1660 - learning_rate: 2.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9714 - loss: 0.0896 - val_accuracy: 0.9560 - val_loss: 0.1765 - learning_rate: 2.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9738 - loss: 0.0764 - val_accuracy: 0.9548 - val_loss: 0.1870 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9777 - loss: 0.0889 - val_accuracy: 0.9560 - val_loss: 0.1727 - learning_rate: 4.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9796 - loss: 0.0857 - val_accuracy: 0.9560 - val_loss: 0.1719 - learning_rate: 4.0000e-05\n",
      "Training CNN with optimizer=adam, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.5869 - loss: 1.0159 - val_accuracy: 0.7106 - val_loss: 0.5387 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7428 - loss: 0.5213 - val_accuracy: 0.8449 - val_loss: 0.3399 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8697 - loss: 0.3108 - val_accuracy: 0.9109 - val_loss: 0.2348 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9135 - loss: 0.2062 - val_accuracy: 0.9133 - val_loss: 0.2154 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9293 - loss: 0.2135 - val_accuracy: 0.8926 - val_loss: 0.2431 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9167 - loss: 0.2024 - val_accuracy: 0.9328 - val_loss: 0.2077 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9533 - loss: 0.1380 - val_accuracy: 0.9341 - val_loss: 0.1866 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8715 - loss: 0.8243 - val_accuracy: 0.6703 - val_loss: 0.5545 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7183 - loss: 0.5342 - val_accuracy: 0.8303 - val_loss: 0.3788 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8332 - loss: 0.3849 - val_accuracy: 0.8730 - val_loss: 0.3187 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8739 - loss: 0.3295 - val_accuracy: 0.8926 - val_loss: 0.2782 - learning_rate: 2.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.8831 - loss: 0.2777 - val_accuracy: 0.9133 - val_loss: 0.2498 - learning_rate: 2.0000e-04\n",
      "Training CNN with optimizer=sgd, dropout=0.3, learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.4980 - loss: 1.2216 - val_accuracy: 0.5201 - val_loss: 0.6864 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6018 - loss: 0.6692 - val_accuracy: 0.6386 - val_loss: 0.6353 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6425 - loss: 0.6338 - val_accuracy: 0.6825 - val_loss: 0.5976 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6936 - loss: 0.5981 - val_accuracy: 0.6935 - val_loss: 0.5486 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7192 - loss: 0.5507 - val_accuracy: 0.7924 - val_loss: 0.4913 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7781 - loss: 0.4920 - val_accuracy: 0.8278 - val_loss: 0.4484 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7646 - loss: 0.5136 - val_accuracy: 0.7094 - val_loss: 0.4904 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7846 - loss: 0.4682 - val_accuracy: 0.8352 - val_loss: 0.4097 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8047 - loss: 0.4773 - val_accuracy: 0.7241 - val_loss: 0.5105 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7149 - loss: 0.6616 - val_accuracy: 0.7277 - val_loss: 0.4923 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6862 - loss: 2.8619 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5278 - loss: 7.5275 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0020\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5100 - loss: 7.8114 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0020\n",
      "Training CNN with optimizer=sgd, dropout=0.3, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5311 - loss: 0.9193 - val_accuracy: 0.6422 - val_loss: 0.6642 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5965 - loss: 0.6636 - val_accuracy: 0.6288 - val_loss: 0.6323 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6431 - loss: 0.6330 - val_accuracy: 0.6996 - val_loss: 0.5947 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6835 - loss: 0.5916 - val_accuracy: 0.6154 - val_loss: 0.5869 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7223 - loss: 0.5465 - val_accuracy: 0.6752 - val_loss: 0.5551 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7554 - loss: 0.5081 - val_accuracy: 0.6850 - val_loss: 0.5229 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7700 - loss: 0.4776 - val_accuracy: 0.8144 - val_loss: 0.4363 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7653 - loss: 1.1424 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5141 - loss: 7.7460 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5244 - loss: 7.5828 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5299 - loss: 7.4947 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0020\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5211 - loss: 7.6353 - val_accuracy: 0.5531 - val_loss: 7.1244 - learning_rate: 0.0020\n",
      "Training CNN with optimizer=sgd, dropout=0.5, learning_rate=0.001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5019 - loss: 0.9149 - val_accuracy: 0.5543 - val_loss: 0.6827 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5640 - loss: 0.6849 - val_accuracy: 0.5568 - val_loss: 0.6521 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6097 - loss: 0.6577 - val_accuracy: 0.6349 - val_loss: 0.6186 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6447 - loss: 0.6190 - val_accuracy: 0.7375 - val_loss: 0.5749 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7044 - loss: 0.5699 - val_accuracy: 0.7912 - val_loss: 0.5317 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6876 - loss: 0.5962 - val_accuracy: 0.7766 - val_loss: 0.5301 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7526 - loss: 0.5265 - val_accuracy: 0.7717 - val_loss: 0.4812 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7508 - loss: 0.4947 - val_accuracy: 0.8254 - val_loss: 0.4564 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7917 - loss: 0.4516 - val_accuracy: 0.8083 - val_loss: 0.4314 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8066 - loss: 0.4361 - val_accuracy: 0.8327 - val_loss: 0.4033 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7978 - loss: 0.4489 - val_accuracy: 0.7424 - val_loss: 0.4605 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8132 - loss: 0.4171 - val_accuracy: 0.8278 - val_loss: 0.4168 - learning_rate: 0.0100\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8286 - loss: 0.4041 - val_accuracy: 0.7900 - val_loss: 0.4324 - learning_rate: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8242 - loss: 0.3932 - val_accuracy: 0.8632 - val_loss: 0.3420 - learning_rate: 0.0020\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8283 - loss: 0.3824 - val_accuracy: 0.8632 - val_loss: 0.3506 - learning_rate: 0.0020\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8546 - loss: 0.3470 - val_accuracy: 0.8694 - val_loss: 0.3466 - learning_rate: 0.0020\n",
      "Epoch 17/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8344 - loss: 0.3689 - val_accuracy: 0.8632 - val_loss: 0.3469 - learning_rate: 0.0020\n",
      "Epoch 18/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8584 - loss: 0.3553 - val_accuracy: 0.8620 - val_loss: 0.3367 - learning_rate: 4.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8566 - loss: 0.3482 - val_accuracy: 0.8559 - val_loss: 0.3346 - learning_rate: 4.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8341 - loss: 0.3748 - val_accuracy: 0.8559 - val_loss: 0.3336 - learning_rate: 4.0000e-04\n",
      "Training CNN with optimizer=sgd, dropout=0.5, learning_rate=0.0001\n",
      "Epoch 1/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5211 - loss: 1.0724 - val_accuracy: 0.5617 - val_loss: 0.6720 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5616 - loss: 0.6807 - val_accuracy: 0.6325 - val_loss: 0.6685 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6137 - loss: 0.6615 - val_accuracy: 0.6288 - val_loss: 0.6360 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6361 - loss: 0.6381 - val_accuracy: 0.6496 - val_loss: 0.6240 - learning_rate: 0.0100\n",
      "Epoch 5/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6726 - loss: 0.6043 - val_accuracy: 0.6496 - val_loss: 0.5886 - learning_rate: 0.0100\n",
      "Epoch 6/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.6858 - loss: 0.5766 - val_accuracy: 0.6960 - val_loss: 0.5383 - learning_rate: 0.0100\n",
      "Epoch 7/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7333 - loss: 0.5444 - val_accuracy: 0.6740 - val_loss: 0.5346 - learning_rate: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7405 - loss: 0.5128 - val_accuracy: 0.7302 - val_loss: 0.5205 - learning_rate: 0.0100\n",
      "Epoch 9/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7364 - loss: 0.5548 - val_accuracy: 0.7363 - val_loss: 0.5286 - learning_rate: 0.0100\n",
      "Epoch 10/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7634 - loss: 0.5140 - val_accuracy: 0.5556 - val_loss: 0.6169 - learning_rate: 0.0100\n",
      "Epoch 11/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7207 - loss: 0.5398 - val_accuracy: 0.7973 - val_loss: 0.4956 - learning_rate: 0.0100\n",
      "Epoch 12/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7745 - loss: 0.4971 - val_accuracy: 0.8254 - val_loss: 0.4304 - learning_rate: 0.0100\n",
      "Epoch 13/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7856 - loss: 0.4538 - val_accuracy: 0.8254 - val_loss: 0.4002 - learning_rate: 0.0100\n",
      "Epoch 14/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7975 - loss: 0.4548 - val_accuracy: 0.8266 - val_loss: 0.4329 - learning_rate: 0.0100\n",
      "Epoch 15/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8141 - loss: 0.4294 - val_accuracy: 0.8486 - val_loss: 0.3733 - learning_rate: 0.0100\n",
      "Epoch 16/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8356 - loss: 0.3919 - val_accuracy: 0.8413 - val_loss: 0.4514 - learning_rate: 0.0100\n",
      "Epoch 17/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.8108 - loss: 0.4320 - val_accuracy: 0.7521 - val_loss: 0.4918 - learning_rate: 0.0100\n",
      "Epoch 18/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8206 - loss: 0.4055 - val_accuracy: 0.7021 - val_loss: 0.5386 - learning_rate: 0.0100\n",
      "Epoch 19/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7314 - loss: 0.5270 - val_accuracy: 0.7961 - val_loss: 0.4647 - learning_rate: 0.0020\n",
      "Epoch 20/20\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.7938 - loss: 0.4493 - val_accuracy: 0.8315 - val_loss: 0.3881 - learning_rate: 0.0020\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(optimizer='adam', dropout_rate=0.5, learning_rate=1e-3):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 1)),  # First convolutional layer\n",
    "        MaxPooling2D(2,2),  # Pooling to reduce spatial dimensions\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu'),  # Second convolutional layer\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Conv2D(128, (3,3), activation='relu'),  # Third convolutional layer\n",
    "        MaxPooling2D(2,2),\n",
    "        \n",
    "        Flatten(),  # Flatten feature maps into a single vector\n",
    "        Dense(128, activation='relu'),  # Fully connected layer\n",
    "        Dropout(dropout_rate),  # Dropout to reduce overfitting\n",
    "        Dense(1, activation='tanh')  # Output layer for binary classification\n",
    "    ])\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer! Choose 'adam' or 'sgd'.\")\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train CNN with different hyperparameters\n",
    "optimizers = ['adam', 'sgd']  # Optimizer variations\n",
    "dropoutValues = [0.3, 0.5]  # Dropout variations\n",
    "learningRates = [1e-3, 1e-4]  # Learning rate variations\n",
    "\n",
    "for opt in optimizers:\n",
    "    for dropout in dropoutValues:\n",
    "        for learning in learningRates:\n",
    "            print(f\"Training CNN with optimizer={opt}, dropout={dropout}, learning_rate={learning}\")\n",
    "            model = build_cnn(optimizer=opt, dropout_rate=dropout, learning_rate=learning)\n",
    "\n",
    "            # Callbacks: Stop training early if no improvement & adjust learning rate\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
    "            ]\n",
    "\n",
    "        # Train the model\n",
    "            model.fit(xTrainCNN,yTrainCNN,  validation_data=(xTestCNN, yTestCNN), epochs=20, callbacks=callbacks, verbose=1)\n",
    "\n",
    "        # Save the trained model\n",
    "            model.save(f'tanh_face_mask_classifier_{opt}_{dropout}_{learning}.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8835 - loss: 0.2973\n",
      "CNN (Optimizer=adam, Dropout=0.3, learning = 0.001) Accuracy: 0.8840\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9474 - loss: 0.1358\n",
      "CNN (Optimizer=adam, Dropout=0.3, learning = 0.0001) Accuracy: 0.9548\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9526 - loss: 0.1593 \n",
      "CNN (Optimizer=adam, Dropout=0.5, learning = 0.001) Accuracy: 0.9487\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9263 - loss: 0.1944\n",
      "CNN (Optimizer=adam, Dropout=0.5, learning = 0.0001) Accuracy: 0.9341\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8351 - loss: 0.4237\n",
      "CNN (Optimizer=sgd, Dropout=0.3, learning = 0.001) Accuracy: 0.8352\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8014 - loss: 0.4583\n",
      "CNN (Optimizer=sgd, Dropout=0.3, learning = 0.0001) Accuracy: 0.8144\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8410 - loss: 0.3514\n",
      "CNN (Optimizer=sgd, Dropout=0.5, learning = 0.001) Accuracy: 0.8559\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8435 - loss: 0.3899 \n",
      "CNN (Optimizer=sgd, Dropout=0.5, learning = 0.0001) Accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "for opt in optimizers:\n",
    "    for dropout in dropoutValues:\n",
    "        for learning in learningRates:\n",
    "            model = keras.models.load_model(f'tanh_face_mask_classifier_{opt}_{dropout}_{learning}.keras')\n",
    "            testLoss, testAccuracy = model.evaluate(xTestCNN, yTestCNN)\n",
    "            print(f\"CNN (Optimizer={opt}, Dropout={dropout}, learning = {learning}) Accuracy: {testAccuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparing the three results\n",
    "<h5> Only the best CNN Model was taken into consideration for comparision with traditional approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.884004884004884\n",
      "MLP Accuracy: 0.9194139194139194\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9691 - loss: 0.1318\n",
      "CNN (optimizer=adam, Dropout=0.5, learning = 0.0001) Accuracy: 0.9683\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Accuracy:\", accuracy_score(yTest, yPredSVM))  # Print accuracy of SVM\n",
    "print(\"MLP Accuracy:\", accuracy_score(yTest, yPredMLP))  # Print accuracy of MLP\n",
    "model = keras.models.load_model(f'face_mask_classifier_{'adam'}_{0.5}_{0.0001}.keras')\n",
    "testLoss, testAccuracy = model.evaluate(xTestCNN, yTestCNN)\n",
    "print(f\"CNN (optimizer={'adam'}, Dropout={0.5}, learning = {0.0001}) Accuracy: {testAccuracy:.4f}\") #Print accuracy of CNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLTutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
